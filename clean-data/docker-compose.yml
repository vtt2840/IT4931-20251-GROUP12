version: "3.8"

networks:
  bigdata-network:
    driver: bridge

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    networks: [bigdata-network]
    environment: { ZOOKEEPER_CLIENT_PORT: 2181 }

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    networks: [bigdata-network]
    ports: ["9092:9092", "29092:29092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on: [zookeeper]

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    networks: [bigdata-network]
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    ports: ["9870:9870", "9000:9000"]
    volumes: [hadoop_namenode:/hadoop/dfs/name]

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    networks: [bigdata-network]
    environment:
      - SERVICE_PRECONDITION=hadoop-namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    depends_on: [hadoop-namenode]
    volumes: [hadoop_datanode:/hadoop/dfs/data]

  hdfs-setup:
    image: bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-setup
    networks: [bigdata-network]
    depends_on:
      - hadoop-namenode
      - hadoop-datanode
    volumes:
      - ./scripts/init-hdfs.sh:/init-hdfs.sh
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    command: ["/bin/bash", "/init-hdfs.sh"]

  ingestion:
    image: vtt2840/ingestion:latest
    container_name: ingestion
    networks: [bigdata-network]
    depends_on:
      hdfs-setup: { condition: service_completed_successfully }
    env_file: .env
    command: ["python", "producer.py"]

  spark-master:
    image: bde2020/spark-master:3.2.1-hadoop3.2
    container_name: spark-master
    networks: [bigdata-network]
    ports: ["8080:8080", "7077:7077"]
    volumes: ["./spark-apps:/spark-apps"]
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    depends_on:
      hdfs-setup: { condition: service_completed_successfully }

  spark-worker:
    image: bde2020/spark-worker:3.2.1-hadoop3.2
    container_name: spark-worker
    networks: [bigdata-network]
    depends_on: [spark-master]
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    volumes: ["./spark-apps:/spark-apps"]

  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scheduler
    networks: [bigdata-network]
    depends_on:
      - spark-master
      - hadoop-namenode
      - kafka
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - SPARK_MASTER=spark://spark-master:7077
      - KAFKA_BROKER=kafka:9092
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./spark-apps:/spark-apps
      - ./logs:/app/logs
    restart: unless-stopped

volumes:
  hadoop_namenode:
  hadoop_datanode:
  