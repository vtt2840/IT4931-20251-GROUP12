from pyspark.sql import SparkSession
from pyspark.sql.functions import col, trim, upper, from_json, year, month, dayofmonth, broadcast, window, avg, max, first
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
import os

# --- Cáº¤U HÃŒNH ---
spark_master = os.getenv("SPARK_MASTER", "spark://spark-master:7077")
kafka_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")
kafka_topic = os.getenv("KAFKA_TOPIC", "air_quality")

# --- LOGIC FIX Lá»–I HDFS (QUAN TRá»ŒNG) ---
# Láº¥y giÃ¡ trá»‹ tá»« biáº¿n mÃ´i trÆ°á»ng, náº¿u khÃ´ng cÃ³ thÃ¬ dÃ¹ng máº·c Ä‘á»‹nh
raw_namenode = os.getenv("HDFS_NAMENODE", "hdfs://hadoop-namenode:9000")

# Ã‰p buá»™c pháº£i cÃ³ hdfs:// á»Ÿ Ä‘áº§u
if not raw_namenode.startswith("hdfs://"):
    # Náº¿u ngÆ°á»i dÃ¹ng quÃªn nháº­p hdfs://, tá»± Ä‘á»™ng thÃªm vÃ o
    if raw_namenode.startswith("http://"):
        # Náº¿u lá»¡ nháº­p http:// thÃ¬ Ä‘á»•i thÃ nh hdfs://
        hdfs_namenode = raw_namenode.replace("http://", "hdfs://")
    else:
        hdfs_namenode = f"hdfs://{raw_namenode}"
else:
    hdfs_namenode = raw_namenode

print(f"ðŸ” HDFS URI CHUáº¨N: {hdfs_namenode}")

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n Output vÃ  Checkpoint
# Äáº£m báº£o Ä‘Æ°á»ng dáº«n báº¯t Ä‘áº§u báº±ng /
output_path = "/data/cleaned_air_quality"
checkpoint_path = "/checkpoints/air_quality"

full_static_path = f"{hdfs_namenode}/data/reference/hanoi_info.csv"
full_output_path = f"{hdfs_namenode}{output_path}"
full_checkpoint_path = f"{hdfs_namenode}{checkpoint_path}"

# Láº¥y Mongo URI
mongo_uri = os.getenv("MONGO_URI")
if not mongo_uri:
    raise ValueError("âŒ Lá»–I: ChÆ°a cÃ³ MONGO_URI trong biáº¿n mÃ´i trÆ°á»ng!")

# 1. KHá»žI Táº O SPARK
spark = SparkSession.builder \
    .appName("Hanoi_AirQuality_Streaming") \
    .master(spark_master) \
    .config("spark.sql.streaming.checkpointLocation", full_checkpoint_path) \
    .config("spark.mongodb.connection.uri", mongo_uri) \
    .config("spark.mongodb.output.uri", mongo_uri) \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# 2. STATIC DATA
try:
    print(f"ðŸ“¥ Äang Ä‘á»c Static Data tá»«: {full_static_path}")
    static_df = spark.read.option("header", "true").csv(full_static_path)
except Exception as e:
    print(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file CSV ({e}). Äang dÃ¹ng dá»¯ liá»‡u giáº£ láº­p.")
    static_df = spark.createDataFrame([("HANOI", "21.0285", "105.8542")], ["city_name", "lat", "lon"])

# 3. SCHEMA
schema = StructType([
    StructField("city", StringType(), True),
    StructField("aqi", IntegerType(), True),
    StructField("co", DoubleType(), True),
    StructField("no2", DoubleType(), True),
    StructField("o3", DoubleType(), True),
    StructField("pm10", DoubleType(), True),
    StructField("pm25", DoubleType(), True),
    StructField("so2", DoubleType(), True),
    StructField("timestamp_utc", TimestampType(), True),
    StructField("timestamp_local", StringType(), True),
    StructField("source", StringType(), True)
])

# 4. Äá»ŒC KAFKA
print("ðŸ“¥ Äang Ä‘á»c Kafka...")
df_raw = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_servers) \
    .option("subscribe", kafka_topic) \
    .option("startingOffsets", "latest") \
    .load()

# 5. Xá»¬ LÃ
df_parsed = df_raw.selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), schema).alias("data")) \
    .select("data.*")

df_clean = (
    df_parsed
    .withWatermark("timestamp_utc", "10 minutes")
    .dropDuplicates(["city", "timestamp_utc"])
    .na.drop(subset=["city", "aqi"])
    .withColumn("city", trim(upper(col("city"))))
    .filter((col("aqi") >= 0) & (col("aqi") <= 1000))
)

df_final = df_clean.withColumn("year", year(col("timestamp_utc"))) \
                   .withColumn("month", month(col("timestamp_utc"))) \
                   .withColumn("day", dayofmonth(col("timestamp_utc")))

# Join
df_enriched = df_final.join(broadcast(static_df), df_final.city == trim(upper(static_df.city_name)), "left").drop(static_df.city_name)

# Aggregate
df_aggregated = df_enriched \
    .groupBy(window(col("timestamp_utc"), "1 hour"), col("city")) \
    .agg(avg("aqi").alias("avg_aqi"), max("pm25").alias("max_pm25"), first("lat").alias("lat"), first("lon").alias("lon"))

# 6. OUTPUT
print(f"ðŸš€ Ghi MongoDB vÃ  HDFS táº¡i: {full_output_path}")

query_mongo = df_aggregated.writeStream \
    .outputMode("update") \
    .format("mongodb") \
    .option("checkpointLocation", f"{full_checkpoint_path}/mongo") \
    .trigger(processingTime='30 seconds') \
    .start()

query_hdfs = df_final.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", full_output_path) \
    .option("checkpointLocation", f"{full_checkpoint_path}/hdfs") \
    .partitionBy("year", "month", "day") \
    .start()

spark.streams.awaitAnyTermination()