
FROM apache/airflow:2.8.1

# BẮT BUỘC: Chuyển sang root để có quyền cài đặt gói phần mềm
USER root

# 1. Cài đặt Java (Bắt buộc để chạy Spark)
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
         openjdk-17-jre-headless \
         procps \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

# 2. Cài đặt Spark Client
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

RUN curl -o spark.tgz https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && tar -xzf spark.tgz \
    && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark \
    && rm spark.tgz

# 3. Tạo thư mục chứa code scripts
RUN mkdir -p /opt/airflow/scripts && chown -R airflow:root /opt/airflow/scripts

# BẮT BUỘC: Chuyển lại về user airflow để chạy ứng dụng
USER airflow

# 4. Cài đặt các thư viện Python cần thiết
# Sửa dòng 34 thành:
RUN pip install --no-cache-dir --default-timeout=1000 pyspark pandas numpy apache-airflow-providers-apache-spark
COPY --chown=airflow:root ./batch /opt/airflow/scripts/

# Thiết lập PYTHONPATH
ENV PYTHONPATH="${PYTHONPATH}:/opt/airflow/scripts"